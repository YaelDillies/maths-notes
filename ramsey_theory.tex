\documentclass{article}

% preamble
\def\npart{III}
\def\nyear{2023}
\def\nterm{Michaelmas}
\def\nlecturer{Dr Julian Sahasrabudhe}
\def\ncourse{Ramsey Theory on Graphs}
\def\draft{Incomplete}

\input{header}

\setcounter{section}{-1}

% and here we go!
\begin{document}
\maketitle

\tableofcontents

\clearpage
\section{Introduction}

\newlec

\begin{notation}
  We write
  \begin{itemize}
    \item $[n] = \{1, \dots, n\}$
    \item $K_n$ for the complete graph on $n$ vertices.
    \item For $X$ a set, $r \in \N$, $X^{(r)} = \{S \subseteq X | |S| = r\}$
    \item $\chi$ for a $k$-coloring of the edges of $K_n$
      \begin{align*}
        \chi : E(K_n) & \to [k] \\
        \chi : E(K_n) & \to \{\text{red}, \text{blue}\} & (\text{if } k = 2)
      \end{align*}
  \end{itemize}
\end{notation}

Ramsey theory is usually concerned with the following question:

\begin{quotation}
  \textit{Can we find some order in enough disorder?}
\end{quotation}

In this course, we will specialise this question to graphs. We are thus interested in the following:

\begin{quotation}
  \textit{What can we say about the structure of an arbitrary $2$-coloring of the edges of $K_n$?}
\end{quotation}

\begin{defi}
  Define the {\bf Ramsey number} $R(\ell, k)$ to be the least $n$ for which every $2$-edge coloring contains either a blue $K_\ell$ or a red $K_k$, and the {\bf diagonal Ramsey number} $R(k) = R(k, k)$ to be the least $n$ for which every $2$-edge coloring contains a monochromatic $K_k$.
\end{defi}

It is unclear that such a $n$ even exists! We shall prove it in due course.

$R(\ell, k) = R(k, \ell)$. By convention, we will usually assume $\ell \le k$.

\begin{eg}
  $R(3) = 6$ because
  \begin{itemize}
    \item The following coloring shows that $R(3) > 5$. TODO: add picture
    \item If we have $6$ vertices, we can pick a vertex $v$. By pigeonhole, three of the neighbors of $v$ are connected to $v$ via the same color, say red. Now either two of those neighbors are connected with a red edge, in which case they form a red triangle with $v$, or they are connected with blue edges to each other, in which case they form a blue triangle. As a way to remember this proof, we encourage you to watch the following music video: \href{https://youtu.be/vE7MW2lk55E}{Everybody's looking for Ramsey}
  \end{itemize}
\end{eg}

\section{Old bounds on \texorpdfstring{$R(k)$}{R(k)}}

\begin{thm}[Erd\H os-Szekeres, 1935]
  $$R(\ell, k) \le \binom{k + \ell - 2}{\ell - 1}$$
  In particular, $R(\ell, k)$ is well-defined.
\end{thm}

\begin{lemma}
  For all $k, \ell \ge 3$,
  $$R(\ell, k) \le \underbrace{R(\ell - 1, k)}_a + \underbrace{R(\ell, k - 1)}_b$$
\end{lemma}
\begin{proof}
  Let $n = a + b$. Pick a vertex $v$. By pigeonhole, either
  \begin{itemize}
    \item $v$ has at least $a$ red neighbors. Either these neighbors contain a red $K_{\ell - 1}$ (in which case we chuck $v$ in), or contain a blue $K_k$ (in which case we already won).
    \item $v$ has at least $b$ blue neighbors. Either these neighbors contain a blue $K_{k - 1}$ (in which case we chuck $v$ in), or contain a red $K_\ell$ (in which case we already won).
  \end{itemize}
\end{proof}

\begin{proof}[Proof of Erd\H os-Szekeres]
  Use that $R(\ell, 2) = \ell$ and induct on $k$ and $\ell$.
\end{proof}

\begin{cor}
  $$R(k) \le \binom{2k} k \le C\frac{4^k}{\sqrt k}$$
  for some constant $C$.
\end{cor}

\subsection{Lower bounds}

Can we find edge colorings on many vertices without a monochromatic $K_k$?
Certainly, we can at least do so on $(k - 1)^2$ vertices.

TODO: Insert figure

This polynomial lower bound is eons away from our exponential upper bound. For quite some time (in the 1930s), people thought that the lower bound was closer to the truth than the upper bound. Surprisingly, it is possible to show an exponential lower bound without actually exhibiting such a coloring!

\begin{thm}[Erd\H os, 1948]
  $$R(k) \ge \frac{k - 1}{e\sqrt 2}2^{\frac k 2}$$
\end{thm}
\begin{fact}
  $$\left(\frac n k\right)^k \le \binom n k \le \left(\frac{en}k\right)^k$$
\end{fact}
\begin{proof}
  Let $n = \ceil{\frac{k - 1}{e\sqrt 2}2^{\frac k 2}}$ and $\chi$ be a random red/blue edge coloring of $K_n$ (each edge is independently colored red or blue with probability $\frac 1 2$). We see that
  \begin{align*}
    \P(\chi \text{ contains a monochromatic } K_k)
    & = \P\left(\bigcup_{S \in [n]^{(k)}} \{S \text{ monochromatic}\}\right) \\
    & \le \binom n k \P([k] \text{ monochromatic}) \\
    & = \binom n k 2^{-\binom k 2 + 1} \\
    & \le 2 \left(\frac{en}k\right)^k 2^{-\frac{k(k - 1)}2} \\
    & = 2 \left(\frac{en}k 2^{-\frac{k - 1}2}\right)^k \\
    & \le 2\left(1 - \frac 1 k\right)^k \\
    & < 1
  \end{align*}
  Hence
  $$2^{\frac k 2} \le R(k) \le 4^k$$
\end{proof}

This proof is remarkable by the fact that it proves that the probability of some object existing is high, without actually constructing such an object. In fact it is still an important open problem to explicitly construct a $K_k$-free edge-coloring of $K_n$ with $n$ exponential in $k$. In other words, {\it even though $K_k$-free edge-colorings are abundant, we don't know how to write down a single one}.

\begin{rmk}
  The use of ``constructive'' here is quite different to that in other areas of mathematics. We do not mean that the proof requires the Law of Excluded Middle or the Axiom of Choice, nor that we do not provide an algorithm to find a graph without monochromatic $K_k$. \\
  Since there are only finitely many red/blue edge-colorings of $K_n$ for a fixed $n$, there trivially is an algorithm to find such a coloring: enumerate them all and try them one by one. Less obviously, there is a procedure to systematically remove any use of the axiom of choice from the proofs of most of the results in this course. Excluded Middle is also redundant since the case splits we consider can be decided in finite time (again, everything is finite). \\
  A more careful definition of ``constructive'' here is about complexity of the description of the object: Erd\H os' lower bound does not provide any better {\it deterministic} algorithm than "Try all edge-colorings", and this has complexity $\Omega\left(2^{\binom n 2}\right)$ (without even accounting for the time it takes to check whether a coloring contains a monochromatic $K_k$). In contrast, we would expect a constructive lower bound to yield an edge-coloring in a polynomial number of operations in $n$.
\end{rmk}

\begin{question}
  What's the base of the exponent here? Is there even such a base?
\end{question}

\newlec

We know
$$R(3, k) \le \binom{k + 1}2 \le (k + 1)^2$$

\begin{defi}
  An {\bf independent set} in a graph is a set of vertices that does not contain an edge. The {\bf independence number} $\alpha(G)$ is the maximum size of an independent set of $G$.
\end{defi}

\begin{defi}[Binomial Random Graph]
  For $n \in \N, 0 \le p \le 1$, we define $G(n, p)$ the probability space of graphs where each edge is independently present with probability $p$.
\end{defi}

\begin{thm}[Erd\H os]
  $$R(3, k) \ge c\left(\frac k{\log k}\right)^{\frac 32}$$
  for some constant $c > 0$.
\end{thm}
\begin{idea}
  We will look at a binomial random graph and choose the parameters so that there are very few red $K_k$ and the number of blue $K_3$ is at most some fixed proportion of $n$. Then we will remove one vertex from each red $K_k$ and one vertex from each blue $K_3$. The resulting graph will have neither and most likely will still contain a fixed proportion of the vertices we started with.
\end{idea}
\begin{proof}
  Change the language. Discuss the blue graph. We are now looking for the maximum number of edges of a graph with no triangles and no independent set of size $k$. \\
  Take $n = \left(\frac k{\log k}\right)^{\frac 32}, p = n^{-\frac 23} = \frac{\log k}k$. Now sample $G \sim G(n, p)$ and define $\tilde G$ to be $G$ with one vertex removed from each triangle and independent set of size $k$. By construction, $K_3 \not\subseteq \tilde G$ and $\alpha(\tilde G) < k$. We will show $\bbE |\tilde G| \ge \frac n2$ using
  $$|\tilde G| \ge n - \#\text{triangles in } G - \#\text{independent sets of size $k$ in } G$$
  First,
  $$\bbE \#\text{triangles in } G
  = \sum_{T \in [n]^(3)} \P(T \text { triangle in } G)
  = \binom n3 p^3 \le \frac{(np)^3}6 = \frac n6$$
  Second,
  \begin{align*}
    \bbE \#\text{independent sets of size $k$ in } G
    & = \binom nk (1 - p)^{\binom k2} \\
    & \le \left(\frac{en}k\right)^k e^{-p\binom k2} \\
    & \sim \left(\frac{en}k e^{-\frac{pk}2}\right)^k \\
    & = \left(\frac{ek^{\frac 32}}{k\log^{\frac 32} k} e^{-\frac{\log k}2}\right)^k \\
    & = \left(\frac e{\log^{\frac 32} k}\right)^k \longrightarrow 0
  \end{align*}
  Hence, for large enough $k$,
  $$\bbE |\tilde G| \ge n - \frac n6 - 1 \ge \frac n2 = \frac 12 \left(\frac k{\log k}\right)^{\frac 32}$$
  By adjusting $c > 0$, we have proved the theorem.
\end{proof}
\begin{rmk}
  The values of $n$ and $p$ come from the constraints
  $$n^3p^3 \ll n, \quad p^{-1}\log p^{-1} \ll k$$
\end{rmk}

We are being wasteful here. Why throw an entire vertex away when we could get away with removing a single edge? Because we might accidentally create an independent set of size $k$. But we can be smarter...

\begin{idea}
  Take a maximal collection of edge-disjoint triangles in $G \sim G(n, p)$ and remove all edges from these triangles.
\end{idea}

\begin{thm}[Erd\H os]
  $$R(3, k) \ge c\left(\frac k{\log k}\right)^2$$
  for some constant $c > 0$.
\end{thm}

\begin{lemma}
  Let $\mathcal F = \{A_1, \dots, A_m\}$ be a family of events in a probability space. Let $\Eps_t$ be the event that $t$ {\it independent} events from $\mathcal F$ occur. then
  $$\P(\Eps_t) \le \frac 1{t!}\left(\sum_{i = 1}^m \P(A_i)\right)^t$$
\end{lemma}
\begin{proof}
  Note that
  $$1_{\Eps_t} \le \frac 1{t!} \sum_{\substack{i \in [m]^t \\ A_{i_1}, \dots, A_{i_t}\text{ independent}}} 1_{A_{i_1}} \dots 1_{A_{i_t}}$$
  So
  \begin{align*}
    \P(\Eps_t)
    & \le \frac 1{t!} \sum_{\substack{i \in [m]^t \\ A_{i_1}, \dots, A_{i_t}\text{ independent}}} \P(A_{i_1}) \dots \P(A_{i_t}) \\
    & \le \frac 1{t!} \sum_{i \in [m]^t} \P(A_{i_1}) \dots \P(A_{i_t}) \\
    & = \frac 1{t!} \left(\sum_{i = 1}^m \P(A_i)\right)^t
  \end{align*}
\end{proof}

\newlec

\begin{idea}
  Instead of killing vertices, we will kill edges. We only need to kill edges from a maximal set of edge-disjoint triangles. For this killing to create an independent set $I$ of size $k$, we must have killed all edges in $I$. But with high probability all sets of size $k$ contain a fixed fraction of the expectation $p\binom k2$ of the number of edges, and having so many edge-disjoint triangles each with two vertices among $k$ fixed vertices is unlikely.
\end{idea}

\begin{lemma}
  Let $n, k \in \N, p \in [0, 1]$ be such that $pk \ge 16\log n$. Then with high probability every subset of size $k$ of $G \sim G(n, p)$ contains at least $\frac{pk^2}8$ edges.
\end{lemma}

\begin{proof}[Proof of Erd\H os' bound]
  We fix $n = \left(\frac{c_1 k}{\log k}\right)^2, p = c_2n^{-\frac 12} = \frac{c_2}{c_1}\frac{\log k}k$. Let $G \sim G(n, p), \mathcal T$ a maximal collection of edge-disjoint triangles in $G$, $\tilde G$ be $G$ with all edges of $\mathcal T$ removed. Note, $\tilde G$ contains no triangle. We show
  $$\P(\alpha(\tilde G) \ge k) < 1$$
  Let $Q$ be the event that every set of $k$ vertices of $G$ contains $\ge \frac{pk^2}8$ edges. Setting $\frac{c_2}{c_1} = 48$, we get
  $$pk = \frac{c_2}{c_1}\frac{\log k}k k = 48\log k > 16\log n$$
  so that $\P(Q) = 1 - o(1)$ by the lemma. Now note that
  $$\P(\alpha(\tilde G) \ge k) \le \P(\alpha(\tilde G) \ge k, Q) + \cancelto 0{\P(Q^c)}$$
  So we focus on $\P(\alpha(\tilde G) \ge k, Q)$. Observe that if $\tilde G$ contains an independent set $I$ of size $k$ and $Q$ holds, then $I$ contains $\ge \frac{pk^2}8$ edges of $G$ by assumption. But $I$ is an independent set in $\tilde G$, so all $\frac{pk^2}8$ edges must have belonged to some triangle $T \in \mathcal T$ and been removed. Therefore
  \begin{align*}
    \P(\alpha(\tilde G) \ge k, Q)
    & \le \P\left(\exists S \in [n]^{(k)}, \mathcal T \text{ meets $S$ in $\ge \frac{pk^2}8$ edges}\right) \\
    & \le \binom n k \P\left(\underbrace{\substack{\text{at least $t$ triangles of $\mathcal T$} \\ \text{meet $[k]$ in at least two vertices}}}_B\right)
  \end{align*}
  where $t = \frac{pk^2}{24}$. Let $\{T_i\}$ be the collection of triangles in $K_n$ that meet $[k]$ in at least two vertices. Let $A_i = \{T_i \subseteq G\}$. Note that if $T_{i_1}, \dots, T_{i_k}$ are edge-disjoint, then $A_{i_1}, \dots, A_{i_k}$ are independent. So
  \begin{align*}
    \P(B)
    & \le \P(\Eps_t) \\
    & \le \frac 1{t!}\left(\sum_{\substack{T_i \subseteq K_n \text{ intersects } [k] \\ \text{ in at least two vertices}}} \P(T_i \subseteq G)\right)^t \\
    & \le \frac 1{t!} (k^2np^3)^t \\
    & \le \left(\frac{ek^2np^3}t\right)^t \\
    & = (24enp^2)^t = (24ec_2^2)^t = e^{-t}
  \end{align*}
  by choosing $c_2 = \frac 1{\sqrt{24} e}$. To finish, observe that
  $$t = \frac{pk^2}{24} = 2k\log k \ge k\log n$$
  Hence
  $$\binom n k \P(B) \le \binom n k e^{-t} \le \left(\frac{en}k e^{-\log n}\right)^k = \left(\frac ek\right)^k \to 0$$
\end{proof}


\subsection{Large deviation inequalities}

Let $Z$ be a gaussian random variable.
$$\P(Z - \bbE Z \ge t) \le e^{\frac{-t}{2\Var Z}}$$
Let $X_1, \dots, X_n$ be iid Bernoulli random variables. We denote this $X_i \sim \Ber(p)$. Write $S_n = X_1 + \dots + X_n$. Note $\bbE S_n = np, \Var(S_n) = np(1 - p)$.

\begin{idea}
  Often, the tail of $S_n$ looks like a gaussian tail.
\end{idea}

\begin{thm}[Chernoff inequality]
  Let $X_1, \dots, X_n \sim \Ber(p)$. Then
  $$\P\left(\abs{S_n - pn} \ge t\right) \le 2\exp\left(\underbrace{-\frac{t^2}{2pn}}_{\text{meat}} + \underbrace{\frac{t^3}{(pn)^2}}_{\text{error term}}\right)$$
\end{thm}

\newlec

\begin{proof}[Proof of the $\frac{pk^2}8$ lemma]
  Using Chernoff on $e(G[[k]])$, namely with $p := p, n := \binom k2, t := \frac{pk^2}4$, we get
  \begin{align*}
    \P(G \text{ fails the statement})
    & = \P\left(\exists S \in [n]^{(k)}, e(G[s]) < \frac{pk^2}8\right) \\
    & \le \binom nk \P\left(e(G[[k]]) < \frac{pk^2}8\right) \\
    & \le \binom nk \P\left(\frac{pk^2}4 < \abs{e(G[[k]]) - p\binom k2}\right) \\
    & \le 2\left(\frac{en}k\right)^k \exp\left(-\frac{pk^2}{16} + \frac 18\right) \\
    & \ll \left(\frac{en}k\right)^k \exp\left(-k\log n\right) \\
    & = \left(\frac ek\right)^k
  \end{align*}
  which tends to 0 as $k$ tends to infinity.
\end{proof}

\subsection{The Local Lemma}

The probabilistic method is like finding the hay in the hay stack. What if we want to find the needle?

\begin{defi}
  Let $\mathcal F = \{A_1, \dots, A_n\}$ be a family of events in a probability space. A {\bf dependency graph} $\Gamma$ is a graph with vertices $\mathcal F$ such that the event $A_i$ is independent of $\sigma(A_j \mid j \not\sim i)$ for all $i \in [n]$.
\end{defi}

\begin{rmks}~
  \begin{itemize}
    \item A dependency graph is not unique.
    \item The complete graph is always a dependency graph.
    \item The empty graph is a dependency graph iff the $A_i$ are globally independent.
  \end{itemize}
\end{rmks}

\begin{thm}[The Local Lemma, symmetric version]
  Let $\mathcal F = \{A_1, \dots, A_n\}$ be a family of events in a probability space, let $\Gamma$ be a dependency graph for $\mathcal F$ with maximum degree $\Delta$. If $\P(A_i) \le \frac 1{e(\Delta + 1)}$ for all $i$, then
  $$\P\left(\Inter_i A_i^c\right) > 0$$
\end{thm}

\begin{thm}[Spencer]
  $$R(k) \ge (1 + o(1)) \frac{\sqrt 2k}e 2^{\frac k2}$$
\end{thm}
\begin{proof}
  Let $n = (1 - \eps)\frac{\sqrt 2k}e2^{\frac k2}$ for some $\eps > 0$. Let $\chi$ be a random edge-coloring of $K_n$ uniformly over all colorings. Define, for $S \in [n]^{(k)}$, the event
  $$A_S = \{S \text{ is monochromatic in } \Z\}$$
  Note we want $\P\left(\Inter_{S \in [n]^{(k)}} A_S^c\right) > 0$. Define the dependency graph $\Gamma$ by
  $$S \sim T \iff 1 < \abs{S \inter T} < k$$
  The maximum degree of $\Gamma$ Is
  $$\Delta = \sum_{t = 2}^{k - 1}\binom kt\binom{n - k}{k - t} = \binom nk - k\binom{n - k}{k - 1} - \binom{n - k}k - 1$$
  To apply the Local Lemma, we just check
  $$\P(A_S) = 2^{-\frac k2 + 1} \le \frac 1{e(\Delta + 1)}$$
\end{proof}

\begin{thm}[Lopsided Local Lemma]
  Let $\mathcal F = \{A_1, \dots, A_n\}$ be a family of events on a probability space, $\Gamma$ a dependency graph for $\mathcal F$, $0 \le x_1, \dots, x_n < 1$ satisfying
  $$\P(A_i) \le x_i \prod_{j \sim i}(1 - x_j)$$
  Then
  $$\P\left(\Inter_i A_i^c\right) \ge \prod_i (1 - x_i) > 0$$
\end{thm}

\begin{thm}[Erd\H os, 1961]
  $$R(3, k) \ge c\left(\frac k{\log k}\right)^2$$
  for some $c > 0$.
\end{thm}
\begin{proof}
  Let $n = \eps^4\left(\frac k{\log k}\right)^2, p = \frac\eps{\sqrt n} = \frac{\log k}{\eps k}, G \sim G(n, p)$. For all $T \in [n]^{(3)}$ and $I \in [n]^{(k)}$, define $A_T = \{T \subseteq G\}$ and $B_I = \{I \text{ independent in }B\}$. We want
  $$\P\left(\Inter_{T \in [n]^{(3)}} A_T^c \inter \Inter_{I \in [n]^{(k)}} B_I^c\right) > 0$$
  Let $\mathcal A = \{A_T\}, \mathcal B = \{B_I\}$. Define $\Gamma$ so that \begin{align*}
    T \sim T' & \iff \abs{T \inter T'} = 2 \\
    T \sim I & \iff 2 \le \abs{I \inter T} \\
    I \sim I' & \iff 2 \le \abs{I \inter I'} < k
  \end{align*}
  $A_T$ has at most $3n$ neighbors in $\mathcal A$ and $3n^{k - 2}$ neighbors in $\mathcal B$. $B_I$ has at most $k^2n$ neighbors in $\mathcal A$ and $k^2n^{k - 2}$ neighbors in $\mathcal B$. We now define $x_T = 3p^3, x_I = n^{-k}$. We have check
  \begin{enumerate}
    \item $p^3 = \P(A_T) \le x_T \prod_{\abs{T \inter T'} = 2} (1 - x_T) \prod_{\abs{I \inter T} \ge 2} (1 - x_I) = 3p^3 (1 - 3p^3)^{3n}(1 - n^{-k})^{3n^{k - 2}}$
    Indeed,
    $$(1 - 3p^3)^{3n}(1 - n^{-k})^{3n^{k - 2}} \ge \exp(-18p^3n - 6n^{-2}) = \exp(-18\eps^2 p - 6n^{-2}) \to 1$$

\newlec

    \item $(1 - p)^{\binom k2} = \P(B_I) \le x_I \prod_{\abs{T \inter I} \ge 2}(1 - x_T) \prod_{2 \le \abs{I \inter I'} < k} (1 - x_{I'}) = n^{-k}(1 - n^{-k})^{k^2k^{k - 2}}(1 - 3p^3)^{k^2n}$
    \begin{align*}
      n^{-k}(1 - n^{-k})^{k^2n^{k - 2}}(1 - 3p^3)^{k^2n}
      & \ge \exp(-k\log n - 2k^2n^{-2} - 6k^2p^3n) \\
      & \ge \exp(-2k\log k - 6k^2p\eps^2 + o(1)) \\
      & \ge \exp\left(-\frac{k\log k}{4\eps} + o(1)\right) \\
      & \ge \exp\left(-p\binom k2\right) \\
      & \ge (1 - p)^{\binom k2}
    \end{align*}
  \end{enumerate}
\end{proof}

\begin{proof}[Proof of the Local Lemma]
  Applying $\P(A \inter B) = \P(A) \P(B \mid A)$ repeatedly, write
  \begin{align*}
    \P\left(\Inter_{i = 1}^n A_i\right)
    & = \prod_{i = 1}^n \P(A_i^c \mid A_1^c \inter \dots \inter A_{i - 1}^c) \\
    & = \prod_{i = 1}^n (1 - \P(A_i \mid A_1^c \inter \dots \inter A_{i - 1}^c))
  \end{align*}
  It is enough to show that $\P(A_i \mid A_1^c \inter \dots \inter A_{i - 1}^c) \le x_i$. We prove
  $$\P(A_i \mid \Inter_{j \in S} A_j^c)$$
  for all $S \subseteq [n]$ by induction:
  \begin{itemize}
    \item $S = \emptyset$. Done by assumption.
    \item Write
    $$I = \Inter_{\substack{j \in S \\ j \not\sim i}} A_j^c, D = \Inter_{\substack{j \in S \\ j \sim i}} A_j^c$$
    So
    \begin{align*}
      \P(A_i \mid I \inter D)
      & = \frac{\P(A_i \inter I \inter D)}{\P(I \inter D)} \\
      & \le \frac{\P(A_i \inter I)}{\P(I \inter D)} \\
      & = \frac{\P(A_i)\P(I)}{\P(I \inter D)} \\
      & = \frac{\P(A_i)}{\P(D \mid I)}
    \end{align*}
    Now, write $D = A_{i_1}^c \inter \dots \inter A_{i_m}^c$ and
    \begin{align*}
    \P(D \mid I)
    & = \prod_{j = 1}^m (1 - \P(A_{i_j} \mid I \inter A_{i_1}^c \inter \dots \inter A_{i_{j - 1}}^c)) \\
    & \ge \prod_{j = 1}^m (1 - x_{i_j}) \\
    & \ge \prod_{j \sim i} (1 - x_j)
    \end{align*}
  \end{itemize}
\end{proof}

\begin{thm}[Lovasz Local Lemma, symmetric version]
  For $\mathcal F = \{A_1, \dots, A_n\}$, $\Gamma$ a dependency graph with maximum degree $\Delta$, if $\P(A_i) \le \frac 1{\Delta + 1}$ for all $i$, then
  $$\P\left(\Inter_i A_i^c\right) \ge \left(1 - \frac 1{e(\Delta + 1)}\right)^n > 0$$
\end{thm}
\begin{proof}
  We use the Lopsided Local Lemma with $x_i = \frac 1{e(\Delta + 1)}$. Note
  $$x_i \prod_{j \sim i} = \frac 1{\Delta + 1}\left(1 - \frac 1{\Delta + 1}\right)^\Delta \ge \frac 1{e(\Delta + 1)} \ge \P(A_i)$$
  So Lopsided Local Lemma applies.
\end{proof}

We now know
$$\frac{ck^2}{(\log k)^2} \le R(3, k) \le (k + 1)^2$$

\begin{thm}[State of the art on $R(3, k)$]
  $$\underbrace{\left(\frac 14 + o(1)\right)\frac{k^2}{\log k}}_{\substack{\text{Fiz Pontiveros} \\ \text{Griffiths} \\ \text{Morris}} + \substack{\text{Bohman} \\ \text{Keevash}}} \le R(3, k) \le \underbrace{(1 + o(1))\frac{k^2}{\log k}}_{\substack{\text{Ajtai} \\ \text{Komlós} \\ \text{Szemerédi}} + \text{Shearer}}$$
\end{thm}

\clearpage

\subsection{Upper bounds on \texorpdfstring{$R(3, k)$}{R(3, k)}}

\newlec

\begin{thm}[Ajtai, Komlós, Szemerédi]
  $$R(3, k) \le c \frac{k^2}{\log k}$$
  for some $c > 0$. In fact, we will see $c = 1 + o(1)$.
\end{thm}

\begin{thm}[Ajtai, Komlós, Szemerédi]
  Let $G$ be a triangle-free graph on $n$ vertices with maximum degree $\Delta$. Then
  $$\alpha(G) \ge c \frac n\Delta \log\Delta$$
  for some absolute constant $c > 0$.
\end{thm}

\begin{rmk}
  For general graphs, we know $\alpha(G) \ge \frac n{\chi(G)} \ge \frac n{d + 1}$ by the naïve greedy algorithm and this is basically best possible. The extra $\log d$ factor will come from tracking how sparse our graph is becoming as we remove vertices from it.
\end{rmk}

We apply a random greedy algorithm to prove the following theorem due to Shearer.

Define
$$f(x) = \frac{x \log x - x + 1}{(x - 1)^2}$$
extending continuously to $[0, 1]$ by $f(0) = 1, f(1) = \frac 12$. We remark that
\begin{itemize}
  \item $f$ is continuous and differentiable.
  \item $f$ is antitone and convex.
  \item $0 < f(x) < 1$
  \item $(x + 1)f(x) = 1 + (x - x^2)f(x)$
\end{itemize}


\begin{thm}[Shearer]
  Let $G$ be a triangle-free graph on $n$ vertices with average degree $d > 0$.
  Then
  $$\alpha(G) \ge nf(d) \ge (1 + o(1)) \frac n\Delta \log\Delta$$
\end{thm}
\begin{proof}
  Induction on $n$.

  Let $x$ a vertex to be chosen later and $G' := G - x - N(x)$. Writing $d'$ the average degree of $G'$ and applying induction to $G'$, we get
  \begin{align*}
    \alpha(G)
    & \ge 1 + \alpha(G') \\
    & \ge 1 + (n - \deg x - 1)f(d') \\
    & \ge 1 + (n - \deg x - 1)(f(d) + (d - d')f'(d)) \\
    & = nf(d) + 1 - (\deg x + 1)f(d) + (d\deg x + d + n(d - d') - d'\deg x - d')f'(d) \\
    & = nf(d) + 1 - (\deg x + 1)f(d) + \left(d\deg x + d - 2\sum_{y \sim x} \deg y\right)f'(d)
  \end{align*}
  where we used that $N(x)$ is an independent set to get that
  \begin{align*}
    n(d - d')
    & = n \left(\frac{2e(G)}n - \frac{2e(G')}{n - \deg x - 1}\right) \\
    & = 2(e(G) - e(G')) - (\deg x + 1)d' \\
    & = 2\sum_{y \sim x} \deg y - (\deg x + 1)d'
  \end{align*}
  So we want to choose $x$ such that
  $$(\deg x + 1)f(d) \le 1 + (d\deg x + d - 2\sum_{y \sim x} \deg y)f'(d)$$
  We average over $x$:
  \begin{align*}
    \E_x \lhs & = (d + 1)f(d) \\
    \E_x \rhs & = 1 + (d^2 + d - 2\E_x\sum_{y \sim x} \deg y)f'(d)
  \end{align*}
  Notice that
  $$\E_x\sum_{y \sim x} \deg y = \frac 1n \sum x\sum_{y \sim x} \deg y = \frac 1n \sum_x \deg x^2 \ge \left(\frac 1n\sum_x \deg x\right)^2$$
  Since $f'(d) \le 0$, we get
  $$\E_x \rhs \ge 1 + (d^2 + d - 2d^2)f'(d) = 1 + (d - d^2)f'(d) = (d + 1)f(d) = \E_x \lhs$$
  So such $x$ exists.
\end{proof}

\begin{proof}[Proof of the AKS bound using Shearer]
  Let $G$ be a graph on $n$ vertices with neither a triangle nor an independent set of size $k$. By triangle freeness, the maximum degree $\Delta$ is strictly less than $k$. By AKS, there is an independent set of size $(1 + o(1))\frac nk \log k$. So
  $$(1 + o(1))\frac nk \log k \le \alpha(G) < k$$
  So $n \le (1 + o(1)) \frac{k^2}{\log k}$
\end{proof}

\newlec

\begin{proof}[Second proof of AKS]
  Let $I$ be an independent set in $G$ sampled uniformly among all independent sets of $G$. We will show
  $$\bbE\abs I \ge c\frac nd\log n$$
  Let $v$ be a vertex. We define the random variable
  $$X_v = d1_{v \in I} + \abs{N(v) \inter I}$$
  For any independent set $I$,
  $$\sum_v X_v \le 2d\abs I$$
  So
  $$\sum_v \E_I X_v \le 2d \E_I \abs I$$
  So we want to show that $\E_I X_v \ge c \log d$ for all $v$. \\
  Let $G' = G - v - N(v)$, find $J$ an independent set in $G'$ minimising $\E_I[X_v \mid I \setminus (N(v) \union \{v\})]$ and let $F = \{w \in N(v) \mid N(w) \inter J = \emptyset\}$ and $t = \abs F$. Note carefully that $I \inter (F \union \{v\})$ is uniform over all independent sets in $G[F \union {v}]$, and that the independent sets of $G[F \union {v}]$ are exactly $\{v\}$ and the $2^t$ subsets of $F$. Hence
  \begin{align*}
    \E_I X_v
    & \ge \E_I[X_v \mid I \setminus (N(v) \union \{v\}) = J] \\
    & = \E_{I \subseteq F \union \{v\}} X_v \\
    & = \frac 1{2^t + 1}d + \frac{2^t}{2^t + 1}\frac t2 \\
    & \ge c \log d
  \end{align*}
  for some $c > 0$ by optimising over $t$.
\end{proof}

\begin{thm}[Ajtai-Komlós-Szemerédi]
  Let $\ell \in \N$. Then for sufficiently large $k \in \N$ we have
  $$R(\ell, k) \le \left(\frac 4{\log k}\right)^{\ell - 2}k^{\ell - 1}$$
\end{thm}

We have seen that the power of $k$ is correct for $\ell = 3$. As of recently, we also know that it is correct for $\ell = 4$.

\clearpage

\section{Hypergraph Ramsey Numbers}

We define $K_n^{(r)}$ to be the {\bf complete $r$-uniform hypergraph} on $n$ vertices. The {\bf $r$-uniform hypergraph Ramsey number} $R^{(r)}(\ell, k)$ is the minimal $n$ such that every edge coloring of $K_n^{(r)}$ contains either a blue $K_\ell^{(r)}$ or a red $K_k^{(r)}$. As before, the {\bf hypergraph diagonal Ramsey number} is $R^{(r)}(k) = R^{(r)}(k, k)$.

\begin{thm}[Erd\H os-Rado]
  $$R^{(3)}(\ell, k) \le 2^{\binom{R(\ell - 1, k - 1)}2}$$
  Eg $R^{(3)}(k) \le 2^{16^k}$.
\end{thm}
\begin{proof}
  Let $t = R(\ell - 1, k - 1)$ and $n = 2^{\binom t2}$. Let $\chi$ be a red/blue edge coloring of $K_n^{(3)}$. Let $v_1, v_2 \in [n]$. Define
  $$A_{1, 2} = \{w \in [n] \mid \chi({v_1, v_2, w}) = c_{1, 2}\}$$
  where $c_{1, 2}$ is the {\bf majority color}, chosen so that
  $$\abs{A_{1, 2}} \ge \frac n2 - 1$$
  Let $v_3 \in A_{1, 2}$. Define
  $$A_{1, 3} = \{w \in A_{1, 2} \mid \chi({v_1, v_3, w}) = c_{1, 3}\}$$
  where $c_{1, 3}$ is the majority color. Now define
  $$A_{2, 3} = \{w \in A_{1, 3} \mid \chi({v_2, v_3, w}) = c_{2, 3}\}$$
  where $c_{2, 3}$ is the majority color, and so on... \\
  After $t$ steps, our world has size $\abs{A_{t - 1, t}} \ge n2^{-\binom t2} \ge 1$. We thus have $\{v_1, \dots, v_t\}$ such that $\chi(\{v_i, v_j, v_k\}) = c_{i, j}$ if $k > i, j$. $c$ is a coloring of $\{v_1, \dots, v_t\}^{(2)}$. By definition of Ramsey, we're done.
\end{proof}

\clearpage

\subsection{Off-diagonal \texorpdfstring{$3$}{3}-uniform hypergraph Ramsey}

\newlec

Erd\H os-Rado gives
$$R^{(3)}(4, k) \le 2^{ck^4}$$

\begin{thm}[Conlon, Fox, Sudakov, 2010]
  $$R^{(3)}(4, k) \le 2^{ck^2}$$
\end{thm}

Erd\H os-Rado makes us shrink our world by a factor of $2$ at every query. Can we ask fewer questions? This suggests the following game.

\begin{defi}[The Ramsey game]
  At each question, we expose a new vertex $v_i$ and get to ask our adversary to expose the color of a collection of edges $v_j v_i$ where $j < i$. Our goal is to force a blue $K_3$ or a red $K_k$ with as few queries as possible.
\end{defi}

\begin{lemma}
  In the Ramsey game, we can force a blue $K_3$ or a red $K_k$ in at most $2k^3$ queries whose answer is ``red'' and $k^2$ queries whose answer is ``blue''.
\end{lemma}
\begin{proof}
  As we expose vertices, we sort them into {\bf levels} $1, 2, 3, \dots$ The first vertex at level $i$ is called the {\bf root} of level $i$ and denoted $r_i$.

  We start by putting $v_1$ into level $1$ and setting $r_1 := v_1$. When we expose vertex $v_i$, we ask for the color of $v_ir_1, \dots, v_ir_r$ until we get replied ``blue''.
  \begin{itemize}
    \item If we get a blue response to $v_ir_j$ for some $j$, stick $v_i$ in level $j$ and expose all edges to previous vertices of level $j$.
    \item If all $v_ir_j$ get replied ``red'', make $v_i$ a new root.
  \end{itemize}
  TODO: Insert picture

  Assuming we have not encountered a blue $K_3$ or red $K_k$, every level contains at most $k$ vertices and there are at most $k$ levels. We have exposed at most $k^2$ red edges and $k$ blue edges in each level, and $k^3$ red edges between levels, so in total at most $2k^3$ red edges and $k^2$ blue edges.
\end{proof}

The idea now is that our adversary wants to reply ``red'' most of the time, so we're willing to shrink our world much more when they reply ``blue''.

\begin{proof}[Proof that $R^{(3)}(4, k) \le 3^{ck^2}$]
  The proof follows the proof of Erd\H os-Rado but we now only refine our world based on the pairs that we query in the Ramsey game. We also have a different rule about when to refine our world to be blue vs red.

  Start with $A_0 = [n]$ where $n = k^{ck^2}$ and define
  $$A_0 \supseteq A_1 \supseteq A_2 \supseteq \dots \supseteq A_n$$
  where $e_j$ is the edge coming from the Ramsey game and
  \begin{align*}
    A_j^B & = \{x \in A_j \mid \chi(e_{j + 1} \union \{x\}) = \text{blue}\} \\
    A_j^R & = \{x \in A_j \mid \chi(e_{j + 1} \union \{x\}) = \text{red}\} \\
    A_{j + 1} & =
    \begin{cases}
      A_j^B & \text{ if } \abs{A_j^B} \ge \frac 1k \abs{A_j} \\
      A_j^R & \text{ if } \abs{A_j^R} \ge \left(1 - \frac 1k\right) \abs{A_j}
    \end{cases}
  \end{align*}
  At the end of time,
  $$\abs{A_m} \ge n\left(\frac 1k\right)^{k^2}\left(1 - \frac 1k\right)^{2k^3} \ge k^{(c - 1)k^2}e^{-2k^2} \ge 1$$
  if we pick $c$ large enough.
\end{proof}

\newlec

What about lower bounds? Let's try the probabilistic method.

Color triples blue with probability $p$.
$$\bbE\left[\#\text{red }K_k^{(3)}\right] = \binom nk (1 - p)^{\binom k3} \le \left(\frac{en}k\right)^k e^{-p\binom k3} = \left(\frac{en}k e^{-\frac{pk^2}6}\right)^k$$
This is nontrivial if $p \gg \frac 1{k^2}$. Then
$$\bbE\left[\#\text{blue }K_4^{(3)}\right] = \binom n4 p^4 \ge \left(\frac n4\right)^4 \gg \left(\frac n{k^2}\right)^4$$
So the (naïve) probabilistic approach looks useless for anything better than polynomial in $k$.

\begin{thm}
  $$R^{(3)}(4, k) \ge 2^{\frac{k - 1}2}$$
\end{thm}
\begin{proof}
  Let $n = 2^{\frac{k - 1}2} - 1$ and $T$ a random tournament on $[n]$. For $x, y \in [n]$ distinct, define
  $$\chi(\{x, y, z\}) =
  \begin{cases}
    \text{blue} & \text{if $x, y, z$ oriented} \\
    \text{red} & \text{if $x, y, z$ acyclic}
  \end{cases}$$
  TODO: Insert oriented and acyclic pictures
  \begin{obs}
    A tournament on $4$ points has at least one transitive triple.
  \end{obs}
  This immediately implies there is no blue $K_4^{(3)}$.
  \begin{obs}
    If $K$ is a tournament where every triple is transitive, then $K$ itself is transitive.
  \end{obs}
  But
  \begin{align*}
    \bbE \#\text{transitive tournament of size } k
    & = \binom nk k! 2^{-\binom k2} \\
    & \le n^k 2^{-\binom k2} \\
    & = \left(n2^{-\frac{k - 1}2}\right)^n \to 0
  \end{align*}
  Hence
  $$\P\left(\chi \text{ has no red } K_k^{(3)}\right) = \P(T \text{ has no transitive tournament of size } k) > 0$$
\end{proof}

\begin{nthm}[Conlon, Fox, Sudakov, 2010]
  $$R^{(3)}(4, k) \ge k^{\frac k5}$$
  for sufficiently large $k$.
\end{nthm}
\begin{proof}[Proof (Stepping up)]
  Set $n = k^{\frac k5}, r = R(3, \frac k4) - 1$. Let $\theta$ be an edge coloring of $K_r$ with no blue $K_3$ or red $K_{\frac k4}$. Now let $\sigma$ be a random edge coloring in $r$ colors. For $x < y < z$, define
  $$\chi(\{x, y, z\}) =
  \begin{cases}
    \theta(\{\sigma(xy), \sigma(xz)\}) & \text{ if } \sigma(xy) \ne \sigma(xz) \\
    \text{ red} & \text{ if } \sigma(xy) = \sigma(xz)
  \end{cases}$$
  \begin{idea}
    The fact that $\theta$ has no blue triangle will imply that $\chi$ has no blue $K_4^{(3)}$. The fact that $\theta$ has no red $K_{\frac k4}$ will imply that $\chi$ has no red $K_k^{(3)}$ with high probability.
  \end{idea}
  Assume that $x < y_1 < y_2 < y_3$ are the vertices of a blue $K_4^{(3)}$. Note that $\sigma(xy_1)$, $\sigma(xy_2)$, $\sigma(xy_3)$ are distinct. So $\sigma(xy_1)\sigma(xy_2), \sigma(xy_2)\sigma(xy_3), \sigma(xy_3)\sigma(xy_1)$ are the edges of a triangle in $K_r$ which is blue in $\sigma$. Contradiction.
  
  TODO: Insert figure

  Now assume that $K = \{x_1, \dots, x_k\}$ is a red $K_k^{(3)}$ in $\chi$. For each $i \in [k]$,
  $$\abs{\{\sigma(x_ix_j) \mid i < j\}} < \frac k4$$
  as otherwise one can find $i < j_1 < \dots < j_{\frac k4}$ such that $\sigma(x_ix_{j_1}), \dots, \sigma(x_ix_{j_{\frac k4}})$ are all distinct, meaning that they are vertices of a red $K_{\frac k4}$ in $\theta$.
  
  TODO: Insert figure

  Call such a set $K \in [n]^{[k]}$ {\bf sad}. We consider
  \begin{align*}
    \bbE \#\text{ sad sets}
    & \le n^k \prod_{i = 1}^k \binom r{\frac k4} \left(\frac k{4r}\right)^{k - i} \\
    & = n^k \binom r{\frac k4}^k \left(\frac k{4r}\right)^{\sum_{i = 1}^k k - i} \\
    & \le n^k \left(\frac{4er}k\right)^{\frac{k^2}4} \left(\frac k{4r}\right)^{\frac{k^2}4} \\
    & = \left(n\left(\frac e4 \frac kr\right)^{\frac k4}\right)^k \\
    & \le \left(nk^{-\frac k4 + o(k)}\right)^k \text{ since } r > k^{2 + o(1)} \\
    & < \left(nk^{-\frac k5}\right)^k \\
    & = 1 \text{ since } n = k^{\frac k5}
  \end{align*}
  Hence find some $\sigma$ such that no set is sad. We're done.
\end{proof}

\newlec

\printindex
\end{document}