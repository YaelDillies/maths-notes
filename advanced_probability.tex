\documentclass{article}

% preamble
\def\npart{III}
\def\nyear{2023}
\def\nterm{Michaelmas}
\def\nlecturer{Prof Perla Sousi}
\def\ncourse{advanced Probability}
\def\draft{Incomplete}

\input{header}
\swapnumbers
\reversemarginpar

\usetikzlibrary{positioning, decorations.pathmorphing, decorations.text, calc, backgrounds, fadings}
\tikzset{node/.style = {circle,draw,inner sep=0.8mm}}

\makeindex[intoc]

\setcounter{section}{-1}

\newcommand{\brownian}[5]{% points, advance, rand factor, options, end label
\draw[#4] (0,0)
\foreach \x in {1,...,#1}
{   -- ++(#2,rand*#3)
}
node[right] {#5};
}

\pgfmathsetseed{23}

% and here we go!
\begin{document}
\maketitle

\tableofcontents

\clearpage

\section{Introduction}

\newlec

This course is concerned with advanced topics in modern probability theory. Two examples are

{\bf Martingales} \\
Martingales are processes indexed by discrete time such that
$$M_{n + 1} = M_n + \text{ extra randomness}$$
where
$$\E[\text{extra randomness} | M_n] = 0$$
A typical example is Markov chains.

{\bf Brownian motion} \\
Brownian motion is a continuous version of discrete random walks. It also arises naturally as the scaling limit of such. If $X_1, \dots$ are iid with mean $\mu$ and variance $\sigma^2$ and set $S_n = X_1 + \dots + X_n$, we have several theorems about
$$\frac{S_n}n \mor \mu$$
namely the Law of Large Numbers, the Central Limit Theorem, and Large Deviation results.

If we now set $B_t^{(n)} = \frac{S_{\floor{nt}} - \mu nt}{\sigma\sqrt n}$, we have that $B_t^{(n)}$ tends to Brownian motion as $n \mor \infty$.
\begin{figure}[h]
  \centering
  \begin{tikzpicture}
    \draw[help lines] (0,-2) grid (10,4);
    \brownian{500}{0.02}{0.2}{black}{}
  \end{tikzpicture}
  \caption{Standard Brownian motion}\label{fig:std-brown}
\end{figure}

TODO: Label Gaussian in figure

Recall {\bf Dirichlet's problem}: If $\mathcal D \subseteq \C$ is a simply connected domain and $f : \partial \mathcal D \mor \C$, can we find a harmonic function $u : \mathcal D \mor \C$ equal to $f$ on $\mathcal D$?

Brownian motion lets us define such a $u$ as follows: \\
Start a Brownian motion at $x \in \mathcal D$. Say it first hits the boundary of $\mathcal D$ in $B_T$. Evaluate $f$ at $B_T$. \\
Now take the expectation of the result and define
$$u(x) = \E[f(B_T)]$$
The resulting $u$ is harmonic and clearly equals $f$ on $\mathcal D$.

One can easily see that the corresponding construction in the discrete setting works by conditioning on the first move of the random walk.

TODO: Insert figure

\clearpage

\section{Conditional Expectation}

\subsection{Basic measure theory recap}

\begin{defi}
  A collection $\mathcal F$ of sets in $\Omega$ is a {\bf $\sigma$-algebra} if
  \begin{itemize}
    \item $\empty \in \mathcal F$
    \item If $A \in \mathcal F$, then $A^c \in \mathcal F$
    \item If $A_n \in \mathcal F$, then $\bigcup_n \in \mathcal F$
  \end{itemize}
\end{defi}

\begin{defi}
  $\P : \mathcal P(\mathcal P(\Omega))$ is a {\bf probability measure} if
  \begin{itemize}
    \item $\P(\empty) = 0$
    \item $\P(\Omega) = 1$
    \item When the $A_n$ are disjoint, $\P\left(\bigcup_n\right) = \sum_n \P(A_n)$
  \end{itemize}
\end{defi}

From now on, $\Omega$ will be a set equipped with a $\sigma$-algebra $\mathcal F$ and a probability measure $\P$.

\begin{defi}
  For $\mathcal A \subseteq \mathcal P(\Omega)$, define
  $$\sigma(\mathcal A) = \bigcap \{\mathcal F | \mathcal F \subseteq A \text{ is a $\sigma$-algebra}\}$$
  the smallest $\sigma$-algebra containing $\mathcal A$, aka {\bf $\sigma$-algebra generated by $\mathcal A$}. \\
  The {\bf Borel $\sigma$-algebra} $\mathcal B$ is the $\sigma$-algebra generated by the open sets in $\R$.
\end{defi}

\begin{defi}
  $X : \Omega \mor \R$ is a {\bf random variable} if $X$ is measurable with respect to $\mathcal B$, namely if $X^{-1}(U) \in \mathcal F$ for all opens $U \subseteq \R$.
\end{defi}

If the $X_i$, $i \in I$ are functions $\Omega \mor \R$, we write $\sigma(X_i | i \in I)$ for $\sigma(\{X_i^{-1}(U) | i \in I, U \subseteq \R \text{ open}\})$, the smallest $\sigma$-algebra making all the $X_i$ measurable.

\subsection{Expectiation}

\begin{defi}
  A {\bf simple function} is a function that can be written as a weighted sum of finitely many indicator functions.
\end{defi}

\begin{defi}
  For a simple function $f = \sum_i a_i 1_{A_i}$, we define
  $$\E[f] = \sum_i a_i \P(A_i)$$
  For a nonnegative function $f$, we define
  $$\E[f] = \sup_{g \le f \text{ simple}} \E[g]$$
  For an arbitrary function $f$, write $f = f^+ - f^-$ with $f^+, f^- \ge 0$, and define
  $$\E[f] = \E[f^+] - \E[f^-]$$
  assuming at least one of $\E[f^+], \E[f^-]$ is finite.
\end{defi}

\begin{defi}[Expectation conditional to an event]
  For $A \in \mathcal F$, define
  $$\E[X|A] = \frac{\E[1_A X]}{\P(A)}$$
\end{defi}

\newlec

\newlec

\newlec

\newlec

\newlec

\newlec

\newlec

\newlec

\newlec

\newlec

\begin{nprop}
  Let $X$ be a continuous process and $A$ an open set. Then
  $$T_A = \inf \{t \mid X_t \in A\}$$
  is a stopping time with respect to $\mcF_{t^+}$.
\end{nprop}
\begin{proof}
  We need to show
  $$\{T_A \le t\} \in \mcF_{t^+}$$
  But, for every $s$,
  $$\{T_A < s\} = \Union_{\substack{q \in \Q \\ q < s}} \{X_q \in A\} \in \mcF_s$$
  So
  $$\{T_A \le t\} \in \mcF_{t^+} = \Inter_n \curlybrack{T_A < t + \frac 1n} \in \Inter_n \mcF_{t + \frac 1n} = \mcF_{t^+}$$
\end{proof}

Let $X$ be a càdlàg stochastic process taking values in $\R^+ \mor E$. Write
\begin{align*}
  C(\R^+, E) & \text{ the space of continuous functions} \\
  D(\R^+, E) & \text{ the space of càdlàg functions}
\end{align*}
Endow the spaces $C, D$ with the product $\sigma$-algebra that makes all evaluations measurable. This $\sigma$-algebra is generated by the cylinder sets
$$\curlybrack{\Inter_{s \in J} \{f(s) \in A_s\} \mid J \text{ finite}, A_s \in B}$$
For $A$ in the product $\sigma$-algebra, we write $\mu(A) = \P(X \in A)$ and call $\mu$ the {\bf law} of $A$. For every $J \subseteq \R^+$ finite, write $\mu_J$ for the law of $\prod_{r \in J} X_r$. The measures $\mu_J$ are called the {\bf finite dimensional marginals} of $X$. \\
The $\mu_j$ completely characterise the law $\mu$ This follows from the uniqueness of extension theorem because cylinder sets generate the product $\sigma$-algebra.

\begin{eg}
  Let $X_t = 0$ for all $t$. Let $U \sim \mcU[0, 1]$ and $X'_t = 1_{U = t}$ for all $t$. Both of them have the same finite dimensional marginals, namely Dirac measures at $0$, but the two processes are not equal:
  $$\P(\for t, X_t = 0) = 1, \P(\for t, X'_t = 0) = 0$$
  However,
  $$\P(X_t = X'_t) = 1$$
\end{eg}

\begin{defi}
  For two processes $X$ and $X'$, we say {\bf $X$ is a version of $X'$} if $X_t = X'_t$ ae for all $t$.
\end{defi}

\begin{defi}
  Say a set $A$ is a {\bf null set} if $\mu(A) = 0$. Write $\mcN$ for the collection of all null sets. Define $\tilde\mcF_t = \sigma(\mcF_{t^+}, \mcN)$. If $\tilde\mcF_t = \mcF_t$ for all $t$, we say that {\bf $\mcF$ satisfies the usual conditions}.
\end{defi}

\begin{thm}[Martingale regularisation theorem]
  Let $X$ be a martingale with respect to $\mcF_t$. There exists a càdlàg process $\tilde X$ such that
  $$X_t \aeeq \E[\tilde X_t \mid \mcF_t]$$
  for all $t$ and $\tilde X$ is a martingale with respect to $\tilde\mcF_t$. If $\mcF$ satisfies the usual conditions, then $\tilde X$ is a version of $X$.
\end{thm}

\begin{nlemma}
  Let $f : \Q^+ \mor \R$ be such that $f$ is bounded on bounded sets and
  $$\mcN(a, b, f\restriction_I) < \infty$$
  for all bounded $I$. Then the limits $\lim_{\substack{s \mor t^- \\ s \in \Q^+}} f(s), \lim_{\substack{s \mor t^+ \\ s \in \Q^+}} f(s)$ exist and are finite.
\end{nlemma}
\begin{proof}
  For any $s_n \mor t^+$, $f(s_n)$ converges by the finite upcrossing proof. Further, for any other $t_n \mor t^+$, we can combine the two sequences together to get that $\lim_n f(s_n) = \lim_n f(t_n)$. Since $f$ is bounded on bounded sets, the limits are finite.
\end{proof}

\newlec

\begin{proof}[Proof of the martingale regularisation theorem]
  \begin{goal}
    Define $\tilde X_t = \lim_{\substack{s \mor t^- \\ s \in \Q}} X_s$ on a set of measure $1$ and $0$ elsewhere.
  \end{goal}
  \begin{steps}
    \begin{enumerate}
      \item Show that the limit exists and is finite on a set of measure $1$.
      \item Show that $\tilde X$ is $\tilde F$-measurable and is finite on a set of measure $1$.
      \item Martingale property of $\tilde X$.
      \item Càdlàg property of $\tilde X$.
    \end{enumerate}
  \end{steps}

  {\bf Construct the set of measure $1$} \\
  Let $I \subseteq \Q_+$ be bounded. We want $\P(\sup_{t \in I} \abs{X_t} < \infty) = 1$. Write
  $$\sup_{t \in I} \abs{X_t} = \sup_{\substack{J \subseteq I \\ \text{finite}}} \sup_{t \in J} \abs{X_t}$$
  Let $J = \{j_1, \dots, j_n\} \subseteq I, j_1 < \dots < j_n$ and $K > \sup I$. $X\restriction_J$ is a discrete-time martingale. By Doob's maximum inequality,
  $$\lambda\P\left(\sup_{t \in J} \abs{X_t} \ge \lambda\right) \le \E[\abs{X_{j_n}}] \le \E[\abs{X_K}]$$
  Taking the limit as $J \mor I$,
  $$\lambda\P\left(\sup_{t \in I} \abs{X_t} \ge \lambda\right) \le \E[\abs{X_K}]$$
  So
  $$\P\left(\sup_{t \in I} \abs{X_t} < \infty\right) = 1$$
  Set $I_M = \Q \inter [0, M]$. Then
  $$\P\left(\Inter_{M \in \N} \curlybrack{\sup_{t \in I_M} \abs{X_t} < \infty}\right) = 1$$
  Let $a, b \in \Q, a < b, I \subseteq \Q_+$ bounded. Write
  $$N(a, b, I, x) = \sup_{\substack{J \subseteq I \\ \text{finite}}} N(a, b, J, x)$$
  Let $J = \{j_1, \dots, j_n\} \subseteq I, j_1 < \dots < j_n$ and $K > \sup I$. $X\restriction_J$ is a discrete-time martingale. By Doob's upcrossing inequality,
  $$(b - a)\E[N(a, b, J, X)] \le \E[(X_{j_n} - a)^-] \le \E[(X_k - a)^-]$$
  By monotone convergence, we get 
  $$(b - a)\E[N(a, b, I, X)] \le \E[(X_k - a)^-]$$
  Define
  $$\Omega_0 = \Inter_{M \in \N} \Inter_{\substack{a, b \in \Q \\ a < b}} \curlybrack{\sup_{t \in I_M} \abs{X_t} < \infty} \inter \{N(a, b, I_m, X) < \infty\}$$
  On $\Omega_0$, the lemma tells us that $\lim_{\substack{s \mor t^- \\ s \in \Q_+}} X_s$ exists, and we have $\P(\Omega_0) = 1$. Define
  $$\tilde X_t =
  \begin{cases}
    \lim_{\substack{s \mor t^- \\ s \in \Q_+}} X_s & \text{ on } \Omega_0 \\
    0 & \text{ elsewhere}
  \end{cases}, \tilde\mcF_t = \sigma(\mcF_{t^+}, \mcN)$$
  From the definition, we see that $\tilde X$ is $\tilde\mcF$-adapted.

  {\bf $X_t = \E[\tilde X_t \mid \mcF_t]$ ae} \\
  Let $t_n \mor t^-, t_n \in \Q_+$. Then by definition $\tilde X_t = \lim_n X_{t_n}$ ae. $X_{t_n}$ is a backwards martingale with respect to $\mcF_{t_n}$. So $X_{t_n}$ converges ae and in $L_1$. But then $\E[X_{t_n} \mid \mcF_t]$ converges to both $X_t$ and $\E[\tilde X_t \mid \mcF_t]$, so $X_t = \E[\tilde X_t \mid \mcF_t]$ ae.

  {\bf$\tilde X$ is a martingale} \\
  Let $s < t$. We want $\E[\tilde X_t]$ ae.
  \begin{claim}
    For all random variables $X$ and $\sigma$-algebra $\mcG$,
    $$\E[X \mid \sigma(\mcG, \mcN)] = \E[X \mid \mcG]$$
  \end{claim}
  In our case, this means
  $$\E[X_t \mid \tilde\mcF_s] \aeeq \tilde X_s$$
  and now the 

\printindex
\end{document}