\documentclass{article}

% preamble
\def\npart{III}
\def\nyear{2023}
\def\nterm{Michaelmas}
\def\nlecturer{Prof Perla Sousi}
\def\ncourse{advanced Probability}
\def\draft{Incomplete}

\input{header}
\swapnumbers
\reversemarginpar

\usetikzlibrary{positioning, decorations.pathmorphing, decorations.text, calc, backgrounds, fadings}
\tikzset{node/.style = {circle,draw,inner sep=0.8mm}}

\makeindex[intoc]

\setcounter{section}{-1}

\newcommand{\brownian}[5]{% points, advance, rand factor, options, end label
\draw[#4] (0,0)
\foreach \x in {1,...,#1}
{   -- ++(#2,rand*#3)
}
node[right] {#5};
}

\pgfmathsetseed{23}

% and here we go!
\begin{document}
\maketitle

\tableofcontents

\clearpage

\section{Introduction}

\newlec

This course is concerned with advanced topics in modern probability theory. Two examples are

{\bf Martingales} \\
Martingales are processes indexed by discrete time such that
$$M_{n + 1} = M_n + \text{ extra randomness}$$
where
$$\E[\text{extra randomness} | M_n] = 0$$
A typical example is Markov chains.

{\bf Brownian motion} \\
Brownian motion is a continuous version of discrete random walks. It also arises naturally as the scaling limit of such. If $X_1, \dots$ are iid with mean $\mu$ and variance $\sigma^2$ and set $S_n = X_1 + \dots + X_n$, we have several theorems about
$$\frac{S_n}n \mor \mu$$
namely the Law of Large Numbers, the Central Limit Theorem, and Large Deviation results.

If we now set $B_t^{(n)} = \frac{S_{\floor{nt}} - \mu nt}{\sigma\sqrt n}$, we have that $B_t^{(n)}$ tends to Brownian motion as $n \mor \infty$.
\begin{figure}[h]
  \centering
  \begin{tikzpicture}
    \draw[help lines] (0,-2) grid (10,4);
    \brownian{500}{0.02}{0.2}{black}{}
  \end{tikzpicture}
  \caption{Standard Brownian motion}\label{fig:std-brown}
\end{figure}

TODO: Label Gaussian in figure

Recall {\bf Dirichlet's problem}: If $\mathcal D \subseteq \C$ is a simply connected domain and $f : \partial \mathcal D \mor \C$, can we find a harmonic function $u : \mathcal D \mor \C$ equal to $f$ on $\mathcal D$?

Brownian motion lets us define such a $u$ as follows: \\
Start a Brownian motion at $x \in \mathcal D$. Say it first hits the boundary of $\mathcal D$ in $B_T$. Evaluate $f$ at $B_T$. \\
Now take the expectation of the result and define
$$u(x) = \E[f(B_T)]$$
The resulting $u$ is harmonic and clearly equals $f$ on $\mathcal D$.

One can easily see that the corresponding construction in the discrete setting works by conditioning on the first move of the random walk.

TODO: Insert figure

\clearpage

\section{Conditional Expectation}

\subsection{Basic measure theory recap}

\begin{defi}
  A collection $\mathcal F$ of sets in $\Omega$ is a {\bf $\sigma$-algebra} if
  \begin{itemize}
    \item $\empty \in \mathcal F$
    \item If $A \in \mathcal F$, then $A^c \in \mathcal F$
    \item If $A_n \in \mathcal F$, then $\bigcup_n \in \mathcal F$
  \end{itemize}
\end{defi}

\begin{defi}
  $\P : \mathcal P(\mathcal P(\Omega))$ is a {\bf probability measure} if
  \begin{itemize}
    \item $\P(\empty) = 0$
    \item $\P(\Omega) = 1$
    \item When the $A_n$ are disjoint, $\P\left(\bigcup_n\right) = \sum_n \P(A_n)$
  \end{itemize}
\end{defi}

From now on, $\Omega$ will be a set equipped with a $\sigma$-algebra $\mathcal F$ and a probability measure $\P$.

\begin{defi}
  For $\mathcal A \subseteq \mathcal P(\Omega)$, define
  $$\sigma(\mathcal A) = \bigcap \{\mathcal F | \mathcal F \subseteq A \text{ is a $\sigma$-algebra}\}$$
  the smallest $\sigma$-algebra containing $\mathcal A$, aka {\bf $\sigma$-algebra generated by $\mathcal A$}. \\
  The {\bf Borel $\sigma$-algebra} $\mathcal B$ is the $\sigma$-algebra generated by the open sets in $\R$.
\end{defi}

\begin{defi}
  $X : \Omega \mor \R$ is a {\bf random variable} if $X$ is measurable with respect to $\mathcal B$, namely if $X^{-1}(U) \in \mathcal F$ for all opens $U \subseteq \R$.
\end{defi}

If the $X_i$, $i \in I$ are functions $\Omega \mor \R$, we write $\sigma(X_i | i \in I)$ for $\sigma(\{X_i^{-1}(U) | i \in I, U \subseteq \R \text{ open}\})$, the smallest $\sigma$-algebra making all the $X_i$ measurable.

\subsection{Expectiation}

\begin{defi}
  A {\bf simple function} is a function that can be written as a weighted sum of finitely many indicator functions.
\end{defi}

\begin{defi}
  For a simple function $f = \sum_i a_i 1_{A_i}$, we define
  $$\E[f] = \sum_i a_i \P(A_i)$$
  For a nonnegative function $f$, we define
  $$\E[f] = \sup_{g \le f \text{ simple}} \E[g]$$
  For an arbitrary function $f$, write $f = f^+ - f^-$ with $f^+, f^- \ge 0$, and define
  $$\E[f] = \E[f^+] - \E[f^-]$$
  assuming at least one of $\E[f^+], \E[f^-]$ is finite.
\end{defi}

\begin{defi}[Expectation conditional to an event]
  For $A \in \mathcal F$, define
  $$\E[X|A] = \frac{\E[1_A X]}{\P(A)}$$
\end{defi}

\newlec

\printindex
\end{document}